{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360000, 11)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "emotions = [\"happy\", \"sad\", [\"disgust\", \"disgust2\"], \"angry\", \"fear\", \"surprise\"]\n",
    "dir_path = \"gathering/ja_tweets_sentiment\"\n",
    "size = 60000\n",
    "df = []\n",
    "for i, es in enumerate(emotions):\n",
    "    if isinstance(es, list):\n",
    "        for e in es:\n",
    "            data = shuffle(pd.read_json(join(dir_path, \"{}.json\".format(e)))).iloc[:int(size/len(es))]\n",
    "            data['label'] = i\n",
    "            df.append(data)\n",
    "    else:\n",
    "        data = shuffle(pd.read_json(join(dir_path, \"{}.json\".format(es)))).iloc[:int(size)]\n",
    "        data['label'] = i\n",
    "        df.append(data)\n",
    "\n",
    "df = pd.concat(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/65/98eb38dfa92c4a1414570db03a4b1eb6cf79f35d0d86da8fae117d56d4e3/sentencepiece-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 3.9MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"twitterstream2word2vec/model/sp/sp.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regexs = []\n",
    "regexs.append(re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'))\n",
    "regexs.append(re.compile('@(\\w){1,15}'))\n",
    "\n",
    "def tokenize(data, regexs, sp=sp):\n",
    "    results = []\n",
    "    for d in data:\n",
    "        try:\n",
    "            for regex in regexs:\n",
    "                d = re.sub(regex, \"\", d)\n",
    "            d = ' '.join([l.replace(\"▁\", \"\").replace(\"#\",\"\") for l in sp.EncodeAsPieces(d)])\n",
    "        except:\n",
    "            d = \"\"\n",
    "        results.append(d)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenize(X, regexs, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "max_features=32000\n",
    "maxlen = 280\n",
    "\n",
    "y = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "def preprocess(data, tokenizer, maxlen=280):\n",
    "    return(pad_sequences(tokenizer.texts_to_sequences(data), maxlen=maxlen))\n",
    "\n",
    "X_train = preprocess(X_train, tokenizer)\n",
    "X_test = preprocess(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec.Word2Vec.load(\"twitterstream2word2vec/model/w2v_gensim/word2vec_tweet.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "vocabulary_size = min(len(word_index)+1, max_features)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)\n",
    "        \n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length, EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=6, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202500 samples, validate on 67500 samples\n",
      "Epoch 1/10\n",
      "202500/202500 [==============================] - 35s 174us/step - loss: 1.7235 - acc: 0.3952 - val_loss: 1.4867 - val_acc: 0.4731\n",
      "Epoch 2/10\n",
      "202500/202500 [==============================] - 34s 167us/step - loss: 1.5152 - acc: 0.4459 - val_loss: 1.4519 - val_acc: 0.4771\n",
      "Epoch 3/10\n",
      "202500/202500 [==============================] - 34s 167us/step - loss: 1.4907 - acc: 0.4517 - val_loss: 1.4514 - val_acc: 0.4750\n",
      "Epoch 4/10\n",
      "202500/202500 [==============================] - 34s 168us/step - loss: 1.4890 - acc: 0.4510 - val_loss: 1.4484 - val_acc: 0.4770\n",
      "Epoch 5/10\n",
      "202500/202500 [==============================] - 34s 168us/step - loss: 1.4914 - acc: 0.4502 - val_loss: 1.4515 - val_acc: 0.4752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f8b2a6e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_preds = model.predict(X_test)\n",
    "y_preds = np.argmax(y_preds, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolabels = []\n",
    "for e in emotions:\n",
    "    if isinstance(e, list):\n",
    "        emolabels.append(e[0])\n",
    "    else:\n",
    "        emolabels.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      happy       0.56      0.56      0.56     15007\n",
      "        sad       0.60      0.54      0.57     15012\n",
      "    disgust       0.43      0.27      0.34     15214\n",
      "      angry       0.47      0.56      0.51     14893\n",
      "       fear       0.38      0.40      0.39     14888\n",
      "   surprise       0.42      0.51      0.46     14986\n",
      "\n",
      "avg / total       0.48      0.47      0.47     90000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_preds, target_names=emolabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\tsad\tdisgust\tangry\tfear\tsurprise\n",
      "1.0\t3.0\t45.0\t29.0\t18.0\t4.0\n",
      "63.0\t5.0\t8.0\t4.0\t5.0\t15.0\n",
      "2.0\t44.0\t16.0\t8.0\t27.0\t3.0\n",
      "1.0\t1.0\t9.0\t86.0\t3.0\t1.0\n",
      "0.0\t5.0\t20.0\t7.0\t62.0\t6.0\n",
      "3.0\t5.0\t3.0\t3.0\t11.0\t76.0\n",
      "48.0\t7.0\t9.0\t14.0\t7.0\t15.0\n",
      "13.0\t6.0\t17.0\t35.0\t13.0\t16.0\n",
      "18.0\t8.0\t18.0\t23.0\t19.0\t13.0\n",
      "21.0\t9.0\t14.0\t16.0\t16.0\t23.0\n",
      "19.0\t7.0\t23.0\t30.0\t12.0\t9.0\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"まじきもい、あいつ\",\n",
    "    \"今日は楽しい一日だったよ\",\n",
    "    \"ペットが死んだ、実に悲しい\",\n",
    "    \"ふざけるな、死ね\",\n",
    "    \"ストーカー怖い\",\n",
    "    \"すごい！ほんとに！？\",\n",
    "    \"葉は植物の構成要素です。\",\n",
    "    \"ホームレスと囚人を集めて革命を起こしたい\",\n",
    "    \"数学は科学に用いられます。\",\n",
    "    \"りんごは赤い。\",\n",
    "    \"とうもろこしは食べ物です。\"\n",
    "]\n",
    "\n",
    "targets = preprocess(tokenize(examples, regexs, sp), tokenizer, maxlen=maxlen)\n",
    "print('\\t'.join(emolabels))\n",
    "for i, ds in enumerate(model.predict(targets)):\n",
    "    print('\\t'.join([str(round(100.0*d)) for d in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_2018-08-29-14:12.h5\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"tokenizer_cnn_ja.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
